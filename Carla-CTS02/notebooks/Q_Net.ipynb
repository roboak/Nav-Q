{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5590e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import time as time\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be1616f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our data and labels\n",
    "\n",
    "x_train = np.array([0.2, 0.1, 0.2, 0.14, 0.11, 0.41, 0.55, 0.3, 0.31, 0.6])\n",
    "y_train = np.array([1,1,1,1,1,-1,-1,-1,-1,-1])\n",
    "\n",
    "\n",
    "# We define the circuit we are going to use\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=1)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def simple_qubit_circuit(theta, inputs):\n",
    "    qml.RX(inputs, wires=0)\n",
    "    qml.RY(theta, wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e93c58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.629756\n",
      "0.01999950408935547\n"
     ]
    }
   ],
   "source": [
    "err = 0\n",
    "theta=0.2\n",
    "t = time.time()\n",
    "for x, y in zip(x_train, y_train):\n",
    "    err += (simple_qubit_circuit(theta, x)-y)**2\n",
    "print(err)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7eefce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.732708\n",
      "0.006994962692260742\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "print(sum((simple_qubit_circuit(theta,x_train)-y_train)**2))\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b5c7a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import time as time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=1)\n",
    "@qml.qnode(dev, interface = 'torch')\n",
    "def simple_qubit_circuit(inputs, theta):\n",
    "    qml.RX(inputs, wires=0)\n",
    "    qml.RY(theta, wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        quantum_weights = np.random.normal(0, np.pi)\n",
    "        self.quantum_weights = nn.parameter.Parameter(torch.tensor(quantum_weights,\\\n",
    "                                    dtype=torch.float32,requires_grad=True))\n",
    "        shapes = {\n",
    "            \"theta\": 1\n",
    "        }\n",
    "        self.q = qml.qnn.TorchLayer(simple_qubit_circuit, shapes)\n",
    "        self.linear = nn.Linear(1,1)\n",
    "    \n",
    "    def forward(self, input_value):\n",
    "        out = self.linear(input_value)\n",
    "        print(out)\n",
    "#         out = out.squeeze(1)\n",
    "        return self.q(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07606113",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3765], dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Function ExecuteTapesBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12876\\3742320349.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\qns\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\qns\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function ExecuteTapesBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [1]"
     ]
    }
   ],
   "source": [
    "# x_train = np.array([0.2, 0.1, 0.2, 0.14, 0.11, 0.41, 0.55, 0.3, 0.31, 0.6])\n",
    "# x_train = torch.tensor(x_train)\n",
    "x_train = torch.rand(1).to(torch.float64)\n",
    "x_train = torch.atan(x_train)\n",
    "model = QNet().double()\n",
    "out = model(x_train)\n",
    "out.mean().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4d487cca",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12876\\1692028561.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'backward'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7bd8a941",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksi01\\AppData\\Local\\Temp\\ipykernel_12504\\2901611185.py:2: DeprecationWarning: The package qiskit.providers.ibmq is being deprecated. Please see https://ibm.biz/provider_migration_guide to get instructions on how to migrate to qiskit-ibm-provider (https://github.com/Qiskit/qiskit-ibm-provider) and qiskit-ibm-runtime (https://github.com/Qiskit/qiskit-ibm-runtime).\n",
      "  IBMQ.save_account('dbb6e66a619d886dd48e2e9a93c0d35197315b5a926524e6566ad0e6936274dce110f5fb433ad0d8200e54f0654cce0c7f1c20ba12fad19ac9f4acf1d0f6001d', overwrite=True)\n",
      "C:\\Users\\aksi01\\AppData\\Local\\Temp\\ipykernel_12504\\2901611185.py:2: DeprecationWarning: The qiskit.IBMQ entrypoint and the qiskit-ibmq-provider package (accessible from 'qiskit.providers.ibmq`) are deprecated and will be removed in a future release. Instead you should use the qiskit-ibm-provider package which is accessible from 'qiskit_ibm_provider'. You can install it with 'pip install qiskit_ibm_provider'. Just replace 'qiskit.IBMQ' with 'qiskit_ibm_provider.IBMProvider'\n",
      "  IBMQ.save_account('dbb6e66a619d886dd48e2e9a93c0d35197315b5a926524e6566ad0e6936274dce110f5fb433ad0d8200e54f0654cce0c7f1c20ba12fad19ac9f4acf1d0f6001d', overwrite=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AccountProvider for IBMQ(hub='ibm-q', group='open', project='main')>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qiskit import IBMQ\n",
    "IBMQ.save_account('dbb6e66a619d886dd48e2e9a93c0d35197315b5a926524e6566ad0e6936274dce110f5fb433ad0d8200e54f0654cce0c7f1c20ba12fad19ac9f4acf1d0f6001d', overwrite=True)\n",
    "IBMQ.load_account()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c4cc1476",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"qiskit.ibmq\", wires=1, backend='ibmq_qasm_simulator', shots=1000)\n",
    "@qml.qnode(dev, interface = 'torch')\n",
    "def simple_qubit_circuit(inputs, theta):\n",
    "    qml.RX(inputs, wires=0)\n",
    "    qml.RY(theta, wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        shapes = {\n",
    "            \"theta\": 1\n",
    "        }\n",
    "        self.q = qml.qnn.TorchLayer(simple_qubit_circuit, shapes)\n",
    "    \n",
    "    def forward(self, input_value):\n",
    "        return self.q(input_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb6558e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken for batch operations:  9.492548942565918\n",
      "tensor([0.9400, 0.9640, 0.9400, 0.9640, 0.9800, 0.9040, 0.8120, 0.9360, 0.9160,\n",
      "        0.8080], dtype=torch.float64, grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([0.2, 0.1, 0.2, 0.14, 0.11, 0.41, 0.55, 0.3, 0.31, 0.6])\n",
    "x_train = torch.tensor(x_train)\n",
    "\n",
    "model = QNet()\n",
    "t1 = time.time()\n",
    "out = model(x_train)\n",
    "print(\"time taken for batch operations: \", time.time()-t1)\n",
    "# out2 = []\n",
    "# t2 = time.time()\n",
    "# for x in x_train:\n",
    "#     out2.append(model(x).item())\n",
    "# print(\"time taken for sequential operations: \", time.time()-t2)\n",
    "\n",
    "print(out)\n",
    "# print(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0165a48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Quantum Torch Layer: func=circuit>\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "def encode(n_qubits, inputs):\n",
    "    for wire in range(n_qubits):\n",
    "        qml.RX(inputs[:,3 * wire + 0], wires=wire)\n",
    "        qml.RY(inputs[:,3 * wire + 1], wires=wire)\n",
    "        qml.RZ(inputs[:,3 * wire + 2], wires=wire)\n",
    "\n",
    "# At the moment contains only two rotation gates Ry and Rz\n",
    "def ansatz_1(n_qubits, y_weight, z_weight):\n",
    "    for wire, y_weight in enumerate(y_weight):\n",
    "        qml.RY(y_weight, wires=wire)\n",
    "    for wire, z_weight in enumerate(z_weight):\n",
    "        qml.RZ(z_weight, wires=wire)\n",
    "    for wire in range(n_qubits):\n",
    "        qml.CZ(wires=[wire, (wire + 1) % n_qubits])\n",
    "\n",
    "\n",
    "def get_model(n_qubits=2, n_layers=1, num_sub_layer, ansatz=1):\n",
    "    dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "    data_per_qubit = 3\n",
    "    shapes = {\n",
    "        \"y_weights\": (n_layers, num_sub_layer, n_qubits),\n",
    "        \"z_weights\": (n_layers, num_sub_layer, n_qubits),\n",
    "    }\n",
    "\n",
    "    @qml.qnode(dev, interface='torch')\n",
    "    def circuit(inputs, y_weights, z_weights):\n",
    "\n",
    "        def sublayer(inputs, sub_layer_idx, layer_idx, y_weights, z_weights):\n",
    "            encode(n_qubits, inputs[:, sub_layer_idx *data_per_qubit*n_qubits:(sub_layer_idx+1) *data_per_qubit*n_qubits])\n",
    "            ansatz_1(n_qubits, y_weights[layer_idx][sub_layer_idx], z_weights[layer_idx][sub_layer_idx])\n",
    "        for layer_idx in range(n_layers):\n",
    "            for sub_layer_idx in range(num_sub_layer):\n",
    "                sublayer(inputs, sub_layer_idx, layer_idx, y_weights, z_weights)\n",
    "\n",
    "        # return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "        # TODO: Fix me\n",
    "        return qml.expval(qml.PauliZ(wires=0)), qml.expval(qml.PauliZ(wires=1))\n",
    "    model = qml.qnn.TorchLayer(circuit, shapes)\n",
    "    return model, dev\n",
    "\n",
    "\n",
    "class QuantumNet(nn.Module):\n",
    "    def __init__(self, n_layers, ansatz, num_sub_layer, n_qubits):\n",
    "        super(QuantumNet, self).__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_actions = 3\n",
    "\n",
    "        self.q_layers, self.dev = get_model(n_qubits=self.n_qubits,\n",
    "                                  n_layers=n_layers, num_sub_layer=num_sub_layer, ansatz=ansatz)\n",
    "        self.out_layer = nn.Linear(self.n_qubits, 1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = torch.atan(inputs).to(torch.float64) #(batch_size x 36)\n",
    "        outputs = self.q_layers(inputs).to(torch.float32)\n",
    "        outputs = self.out_layer(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a371b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.rand(10, 12).to(torch.float64)\n",
    "y_train = torch.rand(10, 1).to(torch.float64)\n",
    "out = model(x_train)\n",
    "loss = (y_train-out).mean()\n",
    "optimiser = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "optimiser.zero_grad()\n",
    "loss.backward()\n",
    "optimiser.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35b0fe63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedNetwork(nn.Module):\n",
    "    def __init__(self, hidden_dim=256):\n",
    "        super(SharedNetwork, self).__init__()\n",
    "\n",
    "        # input_shape = [None, 400, 400, 3]\n",
    "        # self.inp_norm = nn.LayerNorm([400, 400])\n",
    "        self.inp_norm = nn.InstanceNorm2d(3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(8, 8), stride=(4, 4))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
    "        self.layer_norm2 = nn.LayerNorm([64, 48, 48])\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=(9, 9), stride=(3, 3))\n",
    "        self.layer_norm4 = nn.LayerNorm([128, 13, 13])\n",
    "        self.conv5 = nn.Conv2d(128, 128, kernel_size=(9, 9), stride=(1, 1))\n",
    "        self.conv6 = nn.Conv2d(128, hidden_dim, kernel_size=(5, 5), stride=(1, 1))\n",
    "        nn.init.orthogonal_(self.conv1.weight, np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.conv2.weight, np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.conv3.weight, np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.conv4.weight, np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.conv5.weight, np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.conv6.weight, np.sqrt(2))\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.fc_norm1 = nn.LayerNorm([hidden_dim])\n",
    "        nn.init.orthogonal_(self.fc1.weight, np.sqrt(2))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.recurrent_layer = nn.LSTM(hidden_dim+4, hidden_dim, batch_first=True)\n",
    "        for name, param in self.recurrent_layer.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param, np.sqrt(2))\n",
    "    def forward(self, obs:torch.tensor, recurrent_cell:torch.tensor, cat_tensor: torch.tensor=None, sequence_length:int=1):\n",
    "        batch_size = obs.size()[0]\n",
    "        obs = self.inp_norm(obs)\n",
    "        x = self.relu(self.conv1(obs))\n",
    "        x = self.relu(self.layer_norm2(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.layer_norm4(self.conv4(x)))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        # print(x.size())\n",
    "        x = self.relu(self.conv6(x))\n",
    "        # x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = x.reshape((batch_size, -1))\n",
    "        x = self.relu(self.fc_norm1(self.fc1(x)))\n",
    "        x = torch.cat((x, cat_tensor), dim=1)\n",
    "        if sequence_length == 1:\n",
    "            # Case: sampling training data or model optimization using sequence length == 1\n",
    "            x, recurrent_cell = self.recurrent_layer(x.unsqueeze(1), recurrent_cell)\n",
    "            x = x.squeeze(1)  # Remove sequence length dimension\n",
    "        else:\n",
    "            # Case: Model optimization given a sequence length > 1\n",
    "            # Reshape the to be fed data to batch_size, sequence_length, data\n",
    "            x_shape = tuple(x.size())\n",
    "            x = x.reshape((x_shape[0] // sequence_length), sequence_length, x_shape[1])\n",
    "\n",
    "            # Forward recurrent layer\n",
    "            x, recurrent_cell = self.recurrent_layer(x, recurrent_cell)\n",
    "\n",
    "            # Reshape to the original tensor size\n",
    "            x_shape = tuple(x.size())\n",
    "            x = x.reshape(x_shape[0] * x_shape[1], x_shape[2])\n",
    "\n",
    "        return x, recurrent_cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f6927f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02eda3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_PPO(nn.Module):\n",
    "    def __init__(self, n_layers=1, ansatz=1, n_qubits=2, hidden_dim=12):\n",
    "        super().__init__()\n",
    "        self.shared_network = SharedNetwork(hidden_dim=hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bw_per_sublayer = n_qubits*3\n",
    "        self.num_sub_layer = math.ceil(hidden_dim/self.bw_per_sublayer)\n",
    "        self.value_network = QuantumNet(n_layers, ansatz, num_sub_layer=self.num_sub_layer,n_qubits=n_qubits)\n",
    "\n",
    "\n",
    "    def forward(self, obs, recurrent_cell, cat_tensor=None, sequence_length:int=1):\n",
    "        obs =obs.permute(0, 3, 1, 2) # 1x3x400x400\n",
    "        value = None\n",
    "        features, (hx, cx) = self.shared_network(obs=obs, recurrent_cell=recurrent_cell, sequence_length=sequence_length, cat_tensor=cat_tensor) # features = 1x32\n",
    "        # Pads the hidden_dim vector\n",
    "        if self.bw_per_sublayer * self.num_sub_layer != self.hidden_dim:\n",
    "            #     Need to do input padding for value_network\n",
    "            padding_len = self.bw_per_sublayer * self.num_sub_layer - self.hidden_dim\n",
    "            features_value = torch.cat((features, torch.zeros((features.shape[0], padding_len)).to(features.device)), dim=1)\n",
    "            value = self.value_network(features_value)\n",
    "        else:\n",
    "            value = self.value_network(features)\n",
    "        return value, (hx, cx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e77f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Quantum Torch Layer: func=circuit>\n"
     ]
    }
   ],
   "source": [
    "PPO_model = Q_PPO()\n",
    "x_train = torch.rand(10, 400, 400, 3)\n",
    "y_train = torch.rand(10, 1)\n",
    "recurrent_cell = (torch.rand(1,2,12), torch.rand(1,2,12))\n",
    "cat_tensor = torch.rand(10, 4)\n",
    "y_out, _ = PPO_model(x_train, recurrent_cell=recurrent_cell, cat_tensor=cat_tensor, sequence_length=5)\n",
    "loss = (y_out - y_train).mean()\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "341d26e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pennylane as qml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "dev = qml.device(\"default.qubit\", wires=1)\n",
    "@qml.qnode(dev, interface = 'torch')\n",
    "def simple_qubit_circuit(inputs, theta):\n",
    "    qml.RX(inputs, wires=0)\n",
    "    qml.RY(theta, wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        quantum_weights = np.random.normal(0, np.pi)\n",
    "        shapes = {\n",
    "            \"theta\": 1\n",
    "        }\n",
    "        self.q = qml.qnn.TorchLayer(simple_qubit_circuit, shapes)\n",
    "    \n",
    "    def forward(self, input_value):\n",
    "        return self.q(input_value)\n",
    "\n",
    "class Q_PPO1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared_network = nn.Linear(5, 1)\n",
    "        self.value_network = QNet()\n",
    "\n",
    "    def forward(self, obs):\n",
    "        features = self.shared_network(obs)   \n",
    "#         features = features.to(torch.float64)\n",
    "        value = self.value_network(features)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6fc329a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Function ExecuteTapesBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [10]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12876\\3868658154.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_out\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\qns\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\qns\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function ExecuteTapesBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [10]"
     ]
    }
   ],
   "source": [
    "PPO_model = Q_PPO1().double()\n",
    "x_train = torch.rand(10, 5).to(torch.float64)\n",
    "y_train = torch.rand(1)\n",
    "y_out = PPO_model(x_train)\n",
    "loss = (y_out - y_train).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2b0d684",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Function ExecuteTapesBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12876\\2824357264.py\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPPO_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_out\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\envs\\qns\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\qns\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Function ExecuteTapesBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [1]"
     ]
    }
   ],
   "source": [
    "PPO_model = Q_PPO1().double()\n",
    "x_train = torch.tensor([0.1, 0.2, 0.45, 0.123, 0.76]).to(torch.float64)\n",
    "y_train = torch.rand(1)\n",
    "y_out = PPO_model(x_train)\n",
    "loss = (y_out - y_train).mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0ba12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
